{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Medical Q&A Chatbot project (using local TF-IDF + LangChain + FAISS + Gradio)**"
      ],
      "metadata": {
        "id": "bfrMGkbMgNv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook answers queries based on the content of a medical PDF you upload.  \n",
        "Built using: `LangChain`, `Gradio`, `FAISS`, and `TF-ID"
      ],
      "metadata": {
        "id": "EO0SlZrkgoD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community faiss-cpu gradio pypdf scikit-learn --quiet\n"
      ],
      "metadata": {
        "id": "Sm1G9MBuLeSX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"medical_article.pdf\")  # Make sure you uploaded or downloaded it\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "vbBgaRGILmbQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from langchain.embeddings import FakeEmbeddings  # Needed for FAISS to work with TFIDF-like vectors\n",
        "\n",
        "# Create \"fake\" embedding to allow vector storage\n",
        "embedding_model = FakeEmbeddings(size=1536)  # arbitrary dimension for compatibility\n",
        "\n",
        "# This will work just fine as a drop-in\n",
        "vectorstore = FAISS.from_documents(chunks, embedding_model)\n"
      ],
      "metadata": {
        "id": "yuke6FSoL3B9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms.fake import FakeListLLM  # Dummy LLM for testing\n",
        "\n",
        "llm = FakeListLLM(responses=[\"This is a mock answer. Replace with real LLM later.\"])\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type=\"stuff\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "mfyRqN2iL4t-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install websockets==10.4 --quiet\n",
        "!pip install uvicorn==0.20.0 --quiet\n",
        "!pip install gradio==3.41.2 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9R4wRJvL6U_",
        "outputId": "048c15a9-90e3-4857-9fcb-d1349edac1de"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.5.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "lg7ZawGSXhRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "gr.close_all()  # 👈 closes any hidden interfaces hogging the port\n"
      ],
      "metadata": {
        "id": "2tN33-OtZoyl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def medical_chatbot(query):\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    result = chain.run(input_documents=docs, question=query)\n",
        "    return result\n",
        "\n",
        "gr.Interface(\n",
        "    fn=medical_chatbot,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a medical question...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"🩺 Medical Q&A Chatbot (Local TF-IDF)\",\n",
        "    description=\"No API, No Transformers — powered by FAISS + LangChain.\"\n",
        ").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "qtLWe_5VZ2zn",
        "outputId": "2d1d5b1e-058c-4950-8fe4-49698299988c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d6f511586cccb9ce04.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d6f511586cccb9ce04.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/medical_article.pdf\")  # 👈 Use the real filename here\n",
        "docs = loader.load()\n"
      ],
      "metadata": {
        "id": "NrEP1c7oaJ0T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "jysCQfdtaaEX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, chunk in enumerate(chunks[:3]):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTFuAvXDacTR",
        "outputId": "3c9c870b-d50b-45f2-c430-88f641797d54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:\n",
            "Pre-train, Prompt, and Predict: A Systematic Survey of\n",
            "Prompting Methods in Natural Language Processing\n",
            "Pengfei Liu\n",
            "Carnegie Mellon University\n",
            "pliu3@cs.cmu.edu\n",
            "Weizhe Yuan\n",
            "Carnegie Mellon University\n",
            "weizhey@cs.cmu.edu\n",
            "Jinlan Fu\n",
            "National University of Singapore\n",
            "jinlanjonna@gmail.com\n",
            "Zhengbao Jiang\n",
            "Carnegie Mellon University\n",
            "zhengbaj@cs.cmu.edu\n",
            "Hiroaki Hayashi\n",
            "Carnegie Mellon University\n",
            "hiroakih@cs.cmu.edu\n",
            "Graham Neubig\n",
            "Carnegie Mellon University\n",
            "gneubig@cs.cmu.edu\n",
            "Abstract\n",
            "This paper surveys and organizes research works in a new paradigm in natural language processing, which\n",
            "we dub “prompt-based learning”. Unlike traditional supervised learning, which trains a model to take in an\n",
            "input x and predict an output y as P(y|x), prompt-based learning is based on language models that model\n",
            "the probability of text directly. To use these models to perform prediction tasks, the original input x is\n",
            "modiﬁed using a template into a textual string prompt x′that has some unﬁlled slots, and then the language\n",
            "model is used to probabilistically ﬁll the unﬁlled information to obtain a ﬁnal string ˆx, from which the\n",
            "ﬁnal output y can be derived. This framework is powerful and attractive for a number of reasons: it allows\n",
            "the language model to be pre-trained on massive amounts of raw text, and by deﬁning a new prompting\n",
            "function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with\n",
            "few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a uniﬁed\n",
            "set of mathematical notations that can cover a wide variety of existing work, and organize existing work\n",
            "along several dimensions, e.g. the choice of pre-trained models, prompts, and tuning strategies. To make\n",
            "the ﬁeld more accessible to interested beginners, we not only make a systematic review of existing works\n",
            "and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website\n",
            "NLPedia–Pretrain\n",
            " including constantly-updated survey, and paperlist.\n",
            "1\n",
            "arXiv:2107.13586v1  [cs.CL]  28 Jul 2021\n",
            "\n",
            "Chunk 2:\n",
            "CONTENTS\n",
            "Contents\n",
            "1 Two Sea Changes in NLP 3\n",
            "2 A Formal Description of Prompting 4\n",
            "2.1 Supervised Learning in NLP . . . . . . 4\n",
            "2.2 Prompting Basics . . . . . . . . . . . . 4\n",
            "2.2.1 Prompt Addition . . . . . . . . 5\n",
            "2.2.2 Answer Search . . . . . . . . . 5\n",
            "2.2.3 Answer Mapping . . . . . . . . 5\n",
            "2.3 Design Considerations for Prompting . 6\n",
            "3 Pre-trained Language Models 8\n",
            "3.1 Training Objectives . . . . . . . . . . . 8\n",
            "3.2 Noising Functions . . . . . . . . . . . . 8\n",
            "3.3 Directionality of Representations . . . . 9\n",
            "3.4 Typical Pre-training Methods . . . . . . 9\n",
            "3.4.1 Left-to-Right Language Model . 9\n",
            "3.4.2 Masked Language Models . . . 10\n",
            "3.4.3 Preﬁx and Encoder-Decoder . . 10\n",
            "4 Prompt Engineering 11\n",
            "4.1 Prompt Shape . . . . . . . . . . . . . . 11\n",
            "4.2 Manual Template Engineering . . . . . 11\n",
            "4.3 Automated Template Learning . . . . . 11\n",
            "4.3.1 Discrete Prompts . . . . . . . . 12\n",
            "4.3.2 Continuous Prompts . . . . . . 12\n",
            "5 Answer Engineering 13\n",
            "5.1 Answer Shape . . . . . . . . . . . . . . 13\n",
            "5.2 Answer Space Design Methods . . . . . 14\n",
            "5.2.1 Manual Design . . . . . . . . . 14\n",
            "5.2.2 Discrete Answer Search . . . . 14\n",
            "5.2.3 Continuous Answer Search . . . 14\n",
            "6 Multi-Prompt Learning 15\n",
            "6.1 Prompt Ensembling . . . . . . . . . . . 15\n",
            "6.2 Prompt Augmentation . . . . . . . . . . 16\n",
            "6.3 Prompt Composition . . . . . . . . . . 16\n",
            "6.4 Prompt Decomposition . . . . . . . . . 17\n",
            "7 Training Strategies for Prompting Methods 17\n",
            "7.1 Training Settings . . . . . . . . . . . . 17\n",
            "7.2 Parameter Update Methods . . . . . . . 17\n",
            "7.2.1 Promptless Fine-tuning . . . . . 18\n",
            "7.2.2 Tuning-free Prompting . . . . . 18\n",
            "7.2.3 Fixed-LM Prompt Tuning . . . 18\n",
            "7.2.4 Fixed-prompt LM Tuning . . . 18\n",
            "7.2.5 Prompt+LM Tuning . . . . . . 19\n",
            "8 Applications 19\n",
            "8.1 Knowledge Probing . . . . . . . . . . . 19\n",
            "8.2 Classiﬁcation-based Tasks . . . . . . . 19\n",
            "8.3 Information Extraction . . . . . . . . . 22\n",
            "8.4 “Reasoning” in NLP . . . . . . . . . . 22\n",
            "8.5 Question Answering . . . . . . . . . . 23\n",
            "8.6 Text Generation . . . . . . . . . . . . . 23\n",
            "8.7 Automatic Evaluation of Text Generation 23\n",
            "8.8 Multi-modal Learning . . . . . . . . . . 23\n",
            "8.9 Meta-Applications . . . . . . . . . . . 23\n",
            "8.10 Resources . . . . . . . . . . . . . . . . 24\n",
            "9 Prompt-relevant Topics 24\n",
            "10 Challenges 27\n",
            "10.1 Prompt Design . . . . . . . . . . . . . 27\n",
            "10.2 Answer Engineering . . . . . . . . . . 28\n",
            "10.3 Selection of Tuning Strategy . . . . . . 28\n",
            "10.4 Multiple Prompt Learning . . . . . . . 28\n",
            "10.5 Selection of Pre-trained Models . . . . 29\n",
            "10.6 Theoretical and Empirical Analysis of\n",
            "Prompting . . . . . . . . . . . . . . . . 29\n",
            "10.7 Transferability of Prompts . . . . . . . 29\n",
            "10.8 Combination of Different Paradigms . . 29\n",
            "10.9 Calibration of Prompting Methods . . . 29\n",
            "11 Meta Analysis 29\n",
            "11.1 Timeline . . . . . . . . . . . . . . . . . 31\n",
            "11.2 Trend Analysis . . . . . . . . . . . . . 31\n",
            "12 Conclusion 31\n",
            "A Appendix on Pre-trained LMs 44\n",
            "A.1 Evolution of Pre-trained LM Parameters 44\n",
            "A.2 Auxiliary Objective . . . . . . . . . . . 44\n",
            "A.3\n",
            " Pre-trained Language Model\n",
            "Families . . . . . . . . . . . . . . . . . 45\n",
            "2\n",
            "\n",
            "Chunk 3:\n",
            "1 Two Sea Changes in NLP\n",
            "Fully supervised learning, where a task-speciﬁc model is trained solely on a dataset of input-output examples for\n",
            "the target task, has long played a central role in many machine learning tasks (Kotsiantis et al., 2007), and natural\n",
            "language processing (NLP) was no exception. Because such fully supervised datasets are ever-insufﬁcient for\n",
            "learning high-quality models, early NLP models relied heavily on feature engineering (Tab. 1 a.; e.g. Lafferty et al.\n",
            "(2001); Guyon et al. (2002); Och et al. (2004); Zhang and Nivre (2011)), where NLP researchers or engineers\n",
            "used their domain knowledge to deﬁne and extract salient features from raw data and provide models with the\n",
            "appropriate inductive bias to learn from this limited data. With the advent of neural network models for NLP, salient\n",
            "features were learned jointly with the training of the model itself (Collobert et al., 2011; Bengio et al., 2013), and\n",
            "hence focus shifted to architecture engineering, where inductive bias was rather provided through the design of\n",
            "a suitable network architecture conducive to learning such features (Tab. 1 b.; e.g. Hochreiter and Schmidhuber\n",
            "(1997); Kalchbrenner et al. (2014); Chung et al. (2014); Kim (2014); Bahdanau et al. (2014); Vaswani et al. (2017)).1\n",
            "However, from 2017-2019 there was a sea change in the learning of NLP models, and this fully supervised\n",
            "paradigm is now playing an ever-shrinking role. Speciﬁcally, the standard shifted to the pre-train and ﬁne-tune\n",
            "paradigm (Tab. 1 c.; e.g. Radford and Narasimhan (2018); Peters et al. (2018); Dong et al. (2019); Yang et al. (2019);\n",
            "Lewis et al. (2020a)). In this paradigm, a model with a ﬁxed2 architecture is pre-trained as a language model (LM),\n",
            "predicting the probability of observed textual data. Because the raw textual data necessary to train LMs is available\n",
            "in abundance, these LMs can be trained on large datasets, in the process learning robust general-purpose features\n",
            "of the language it is modeling. The above pre-trained LM will be then adapted to different downstream tasks by\n",
            "introducing additional parameters and ﬁne-tuning them using task-speciﬁc objective functions. Within this paradigm,\n",
            "the focus turned mainly to objective engineering, designing the training objectives used at both the pre-training and\n",
            "ﬁne-tuning stages. For example, Zhang et al. (2020a) show that introducing a loss function of predicting salient\n",
            "sentences from a document will lead to a better pre-trained model for text summarization. Notably, the main body\n",
            "of the pre-trained LM is generally (but not always; Peters et al. (2019)) ﬁne-tuned as well to make it more suitable\n",
            "for solving the downstream task.\n",
            "Now, as of this writing in 2021, we are in the middle of a second sea change, in which the “pre-train, ﬁne-tune”\n",
            "procedure is replaced by one in which we dub “pre-train, prompt, and predict”. In this paradigm, instead of adapting\n",
            "pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more\n",
            "like those solved during the original LM training with the help of a textual prompt. For example, when recognizing\n",
            "the emotion of a social media post, “I missed the bus today.”, we may continue with a prompt “I felt so ”, and\n",
            "ask the LM to ﬁll the blank with an emotion-bearing word. Or if we choose the prompt “English: I missed the bus\n",
            "today. French: ”), an LM may be able to ﬁll in the blank with a French translation. In this way, by selecting\n",
            "the appropriate prompts we can manipulate the model behavior so that the pre-trained LM itself can be used to\n",
            "predict the desired output, sometimes even without any additional task-speciﬁc training (Tab. 1 d.; e.g. Radford\n",
            "et al. (2019); Petroni et al. (2019); Brown et al. (2020); Raffel et al. (2020); Schick and Sch ¨utze (2021b); Gao\n",
            "et al. (2021)). The advantage of this method is that, given a suite of appropriate prompts, a single LM trained in an\n",
            "entirely unsupervised fashion can be used to solve a great number of tasks (Brown et al., 2020; Sun et al., 2021).\n",
            "However, as with most conceptually enticing prospects, there is a catch – this method introduces the necessity for\n",
            "prompt engineering, ﬁnding the most appropriate prompt to allow a LM to solve the task at hand.\n",
            "This survey attempts to organize the current state of knowledge in this rapidly developing ﬁeld by providing an\n",
            "overview and formal deﬁnition of prompting methods (§2), and an overview of the pre-trained language models that\n",
            "use these prompts (§3). This is followed by in-depth discussion of prompting methods, from basics such as prompt\n",
            "engineering (§4) and answer engineering (§5) to more advanced concepts such as multi-prompt learning methods\n",
            "(§6) and prompt-aware training methods (§7). We then organize the various applications to which prompt-based\n",
            "learning methods have been applied, and discuss how they interact with the choice of prompting method ( §8).\n",
            "Finally, we attempt to situate the current state of prompting methods in the research ecosystem, making connections\n",
            "to other research ﬁelds (§9), suggesting some current challenging problems that may be ripe for further research\n",
            "(§10), and performing a meta-analysis of current research trends (§11).\n",
            "Finally, in order to help beginners who are interested in this ﬁeld learn more effectively, we highlight some\n",
            "systematic resources about prompt learning (as well as pre-training) provided both within this survey and on\n",
            "companion websites:\n",
            "•\n",
            " : A website of prompt-based learning that contains: frequent updates to this survey, related slides, etc.\n",
            "• Fig.1: A typology of important concepts for prompt-based learning.\n",
            "1Even during this stage, there was some use of pre-trained models exempliﬁed by word2vec (Mikolov et al., 2013b,a) and\n",
            "GloVe (Pennington et al., 2014), but they were used for only a limited portion of the ﬁnal model parameters.\n",
            "2This paradigm is less conducive to architectural exploration because (i) unsupervised pre-training allows models to learn\n",
            "with fewer structural priors, and (ii) as pre-training of models is time-consuming, experimenting with structural variants is costly.\n",
            "3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert chunks to plain text\n",
        "chunk_texts = [doc.page_content for doc in chunks]\n",
        "\n",
        "# Create TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(chunk_texts)\n"
      ],
      "metadata": {
        "id": "DcmYMf0caooX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_answer(query):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarity_scores = cosine_similarity(query_vec, tfidf_matrix)\n",
        "\n",
        "    # Get the most relevant chunk\n",
        "    top_idx = similarity_scores.argmax()\n",
        "    return chunk_texts[top_idx]\n"
      ],
      "metadata": {
        "id": "A6ASnY8jarwt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(query):\n",
        "    try:\n",
        "        return retrieve_answer(query)\n",
        "    except:\n",
        "        return \"Sorry, something went wrong. Please try a different question.\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(label=\"query\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"🩺 Medical Q&A Chatbot (Local TF-IDF)\",\n",
        "    description=\"No API, No Transformers — powered by FAISS + LangChain.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "BFo94878atqj",
        "outputId": "2d3fd25e-cde0-486a-a10c-533e9d1c942a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://04d84480c5eaa64bf5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://04d84480c5eaa64bf5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}